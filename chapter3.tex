\chapter{Research Questions}

\atrcomments{DELETE}

To the best of my knowledge, there is little direct correlation between my work on POS tagging and parsing experts to that of the previous work done in the area.
However, my work is comparable to domain adaptation since I create experts to tag and parse heterogeneous datasets. %The work in this is area is largely driven by the unavailability of examples from target domain. 
My work focuses on creating experts using topic modeling which will be able to tag and parse target domain sentences belonging to a specific topic. Compared to POS tagging, there has been significant work on domain adaptation in dependency parsing.
The primary challenge of domain adaptation is unavailability of examples from target domain. The previous work done in this area focuses on analyzing how it affects the problem in question and building systems to bypass the need of having substantial volume of target domain examples.

\cite{dredze2007frustratingly} concluded that domain adaptation is more challenging when there are dissimilarities in annotation schemes between the treebanks.
\cite{blitzer:mcdonald:ea:06} experimented on structural correspondence learning (SCL) which focuses on finding ``frequently occurring" pivot features that occur commonly across domains in the unlabeled data but equally characterize source and target domains. Similar to My work, Blitzer et al.\ used the WSJ as the source and MEDLINE abstracts as the target domain. They established that SCL reaches better results in both POS tagging and parsing than supervised and semi-supervised learning even when there is no training data available on the target domain.

For POS tagging,~\cite{clark:curran:ea:03} applied an agreement-based and a baseline co-training method by using a Markov model tagger and a maximum entropy tagger. In case of the baseline, all the sentences from one tagger are added to train the other whereas in the agreement-based method, both taggers have to reach to the same decision for a sentence to be added to the training.
\cite{kuebler:baucom:11} used a similar concept but with three different taggers and showed that selecting sentences as well as sequences of words for which all taggers agree yield the highest gains.
Sagae and Tsujii~\cite{sagae2007dependency} emulate a single iteration of co-training by using MaxEnt and SVM, selecting the sentences where both models agreed and adding these sentences to the training set. Their approach reached the highest results on the domain adaptation task of CoNLL 2007~\cite{nilsson2007conll}.

Domain adaptation was the task of the CoNll 2007 shared task. 
\cite{attardi2007multilingual} used a tree revision method~\cite{attardi2007tree} that corrects the mistakes caused by the base parser for the target domain. Revision stands for moving a dependency arc to a different head in the dependency tree. They formulated the problem as a supervised classification task using multiclass perceptron, where the set of revision rules is the output space. They use syntactic and morphological properties of a dependency tree. This method is similar to reranking~\cite{charniak2005coarse,collins2005discriminative}, which works by generating $n$-best parses of a given sentence. Then, with the help of global features and a discriminative models, the reranker trains on these parses. \cite{McClosky:2006:ESP:1220835.1220855,McClosky:2006:RSP:1220175.1220218} also showed the effectiveness of using reranking using self training. This is based on Brill's POS tagger~\cite{brill1992simple,Brill:1995:TEL:218355.218367} using transformation error driven learning, where he initializes with the baseline which is the most frequent part of speech tags for a sentence. Then the model corrects the errors by applying the rules repeatedly. This approach has also been implemented by \cite{Nakagawa:2002:SBP:1118771.1118778,Nakagawa:2002:RLA:1073083.1073167} for POS tagging, where they use a second classifier to determine the accuracy of the base parser. My work in question 3 is similar to this concept, since I attempt to correct an output parse from the target.

%\todo{expand and relate and link with reranking and transformation based systems}
Later, \cite{kawahara2008learning} employed a single parser approach using second order MST Parser and combining labeled data from the unknown domain with unlabeled data of the known domain by simple concatenation and judging the efficacy of the resulting most reliable parses. %\todo{expand}
\cite{Finkel:2009:HBD:1620754.1620842} devised a new model for named entity recognition as well as dependency parsing by using hierarchical Bayesian prior. This is influenced by the notion that different domains may have different features which is specific to each domain.  However, instead of applying a constant prior over all the parameters,  a hierarchical Bayesian global is used. This enables sharing of information across domains but also allows to override this information if there is ample evidence. 

\cite{mcclosky2010automatic} designed the problem as ``multiple source parse adaptation", in which a parser was trained on multiple domains and learned the statistics as well as domain differences which affects the parser accuracy. Their parser outperforms the state-of-the-art baselines. This approach is the closest to my work as I create experts based on topics, and each expert learns the specifics of the particular topic with which it is associated. %\todo{may be expand?}

The closest approach to my approach is the one by \cite{plank2011effective}, who employ a similar idea of using topic modeling for creating parsing experts. Their general approach is that, given a sentence from a target domain, whether it is possible to ``select" appropriate training sentences. In other words, their task is to determine in a domain adaption setting which sentences of an out-of-domain training set are the most similar to the test set. Thus, they create a specialized training set for every document they need to parse. The topic distribution in a document is used as features for their similarity metrics. So, their works relies on selecting appropriate features and calculate the similarity between a test sentence and a pool of training sentences. They have used similarity metrics such as KL Divergence, Jensen-Shannon divergence as well as various distance functions such as cosine similarity, euclidean \& Manhattan distances. My approach is more general, since I create experts once and then each time a new sentence comes in, it is assigned to the closest expert by estimating the unigram perplexity. So, I save in terms of effort needed to search the whole training set to find the right fit.

