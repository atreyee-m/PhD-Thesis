\chapter{Corpora}

In this chapter, I elaborate on the corpora I used for the experiments conducted on domain adaptation for POS tagging and dependency parsing. I use three main corpora as a representative of their domains. I discuss this in detail in the next sections. In addition to classical domain adaptation, the goal is also to achieve a more generic form of domain adaptation, i.e., to evaluate if my system can detect domains in a heterogeneous or mixed dataset. Thus, I create an artificial corpus from the corpora, which contains equal representation of two domains. Although all the corpora used in this thesis are annotated in the PTB style, each has a distinctive syntactic style, which ... ~\todo{dont know how to finish this sentence}.

%Since the goal is domain adaptation, the corpora used for the experiments need to be reasonably diverse. 
  
\section{Wall Street Journal section of Penn Treebank (WSJ)}

~\todo{check!!}
\footnote{https://catalog.ldc.upenn.edu/LDC2000T43}
This is the Wall Street Journal section~\citep{Marcus:1994:PTA:1075812.1075835} of the Penn Treebank~\citep{santorini:90}. \textcolor{red}{This corpus contains annotated newspaper articles from WSJ from 1987 - 1989. The corpus is annotated for POS tags and parsed in PTB bracketed style. The dependency trees are then derived using the Penn Converter Tool~\citep{johansson2007a} and the inconsistencies are addressed manually}. I use this corpus as a representative of the newspaper domains. There are various different kinds of news articles from financial news, theater critique, weather, sports, politics, etc. Thus, in addition to contributing as a main domain, we can also exploit the corpus for micro-genres or more fine-grained domains. The sentences in WSJ are reasonably longer with an average length of 24 words. I use the standard split for parsing, which is, 02-21 for training, 22 for test and 23 for validation. Some of the example sentences from WSJ are as given below.  ~\todo{EXAMPLES}

\section{GENIA Corpus}

The GENIA Corpus~\citep{tateisi:tsujii:04} comprises of biomedical abstracts from Medline, and it is annotated on different linguistic levels, including POS tags, syntax, coreference, and events, among others. I use GENIA 1.0 trees~\cite{Ohta:2002:GCA:1289189.1289260} created in the Penn Treebank format\footnote{http://nlp.stanford.edu/~mcclosky/biomedical.html}. The treebank is converted to dependencies using pennconverter~\cite{johansson2007a}. The tagset used in GENIA is based on the Penn Treebank tagset, but it uses the tags for proper names and symbols only in very restricted contexts. This treebank serves as a representative of biomedical domain for my experiments. Clearly, this is very distinct from WSJ in terms of the nature of the content. Some of the sentences from the corpus are shown below.~\todo{EXAMPLES}


\section{CReST Corpus}

The CReST corpus~\citep{eberhard2010indiana} constitutes of natural language dialogues between two individuals performing a cooperative, remote, search task (CReST). This is a multimodal corpus which is annotated for speech signals and their corresponding transcriptions.


% \subsection{Data Sets}

% For our experiments, we use the Wall Street Journal (WSJ) section of
% the Penn Treebank \cite{marcus:kim:ea:94} and the GENIA Corpus \cite{tateisi:tsujii:04}. Both corpora use the Penn
% Treebank POS tagset \cite{santorini:90} with minor differences: The tagset used in
% GENIA is based on the Penn Treebank tagset, but it uses the tags for
% proper names and symbols only in very restricted contexts. 

% For the WSJ corpus, we extract the POS annotation from the
% syntactically annotated corpus. The GENIA Corpus comprises biomedical
% abstracts from Medline, and it is annotated on different
% linguistic levels, including POS tags, syntax, coreference, and
% events, among others. We use GENIA 1.0 trees~\cite{Ohta:2002:GCA:1289189.1289260} created in the Penn Treebank format\footnote{http://nlp.stanford.edu/~mcclosky/biomedical.html}. Both treebanks were converted to dependencies using pennconverter~\cite{johansson2007a}.

% For our experiments, we need a balanced data set, both for the training and
% the test set. Since GENIA is rather small and since there is no standard data split for GENIA, we decided to extract the last 850 sentences for the test set. The remaining 17~181 sentences are used for training. For WSJ, we chose the same number of sentences for both training and the test set, the training sentences are selected randomly from sections 02-21 and the test sentences from section 22.\ignore{SK: is that correct?} 




% \section{Corpus}
% \todo[inline]{EACL+ICON}

% \subsection{Wall Street Journal section of Penn Treebank (WSJ)}
% WSJ~\cite{Marcus:1994:PTA:1075812.1075835} - This is the Wall Street Journal section of the Penn Treebank~\cite{santorini:90}. This dataset consists of newspaper articles/reports from the financial domain as well as reports on politics, weather, etc. This is the representative of the newspaper domain.

% \subsection{Data Sets}


% For our experiments, we use the Wall Street Journal (WSJ) section of the Penn Treebank \cite{marcus:kim:ea:94} and the GENIA Corpus (version 3.02) \cite{tateisi:tsujii:04}. Both corpora use the Penn Treebank POS tagset \cite{santorini:90} with minor differences.%, as described in section \ref{sec:q1}.

% For the WSJ corpus, we extract the POS annotation from the syntactically annotated corpus. The GENIA Corpus comprises biomedical abstracts from Medline, and it is annotated on different linguistic levels, including POS tags, syntax, coreference, and events, among others. We use the POS tagged version. For WSJ, we use the standard data split for parsing:  using sections 02-21 as training data and section 22 as our test set. We reserve section 23 for future parsing expert experiments.

% \subsection{Data Sets}
% In our current work, we focus on two domains: financial news articles and dialogues in a collaborative task.
% We use the Wall Street Journal \cite{Marcus:1994:PTA:1075812.1075835} and the CReST corpus~\cite{eberhard2010indiana} as representative corpora for these two domains. For both corpora, we use teh dependency version: CReST was originally annotated in constituents and dependencies, the Penn Treebank was automatically converted to dependencies using \textit{pennconverter} \cite{johansson2007a}. %Wall Street Journal consists of financial articles, news, etc. CReST corpus, on the other hand, comprises of dialogues recorded during a collaborative task.%natural language dialogues between two people, who carry out a certain task with the help of each other. 
% %%SK check numbers? - x

% These corpora are very dissimilar in nature. On an average, the length of the sentences for WSJ is 24 and 7 for CReST. In terms of linguistic factors, CReST has a subset of WSJ's dependency labels with 10 new labels added. %and 
% %%SK why do we care about POS tags? should that be dep labels? - atr: because I was thinking this might also show how different the corpora are, but its not needed.
% %has 14 different part of speech tags than WSJ. 
% Example sentences from each corpora are shown in table~\ref{tab:samplesentences}. We use WSJ as the source domain and CReST as the target domain.  %We create a parsing model using the MATE parser~\cite{bohnet:2010:PAPERS,bohnet2010very}. We then parse sentences from CReST using this model. %Since the source domain (WSJ) and the target domain (CReST) have differences in dependency label annotation, this introduces some errors in the resulting parse. 

% \begin{table}[!t]
% \centering
% \begin{tabular}{l|l}
% \hline
% WSJ   & \begin{tabular}[c]{@{}l@{}}
% In an Oct. 19 review of " The Misanthrope " at Chicago 's Goodman Theatre \\ ( " Revitalized Classics Take the Stage in Windy City , " Leisure \& Arts ) , \\ the role of Celimene , played by Kim Cattrall , was mistakenly attributed \\ to Christina Haag .\end{tabular} \\ \hline
% CReST & \begin{tabular}[c]{@{}l@{}}
% it's like as soon - like if I were to close the door it's right next to like the \\ bottom of the floor like where the door closes 
% \end{tabular} \\ \hline
% %so I went through some - I went through the second room right? \\ \hline    
% %can I tell you where that is?   
% \end{tabular}
% \caption{Sample sentences from Wall Street Journal (WSJ) \& CReST corpus}
% \label{tab:samplesentences}
% \end{table}



% \paragraph{Target Domain}
% The CReST corpus consists of 23 dialogues that were manually annotated for dependencies. We randomly select 19 dialogues as our training data for the TBL algorithm and the rest as test. 
% Since the system needs to learn rule hypotheses from  incorrect predictions of the source domain parser, 
% we can safely ignore sentences that were parsed completely correctly. For the test data, we use all the sentences from the designated dialogues (with correct and incorrect dependency label predictions). 
% %As a part the training part of Transformation Based Error Driven Learning (TBL), the system needs to learn rule hypotheses from the incorrect dependency label prediction, we work on the sentences which show errors in dependency labels. 
% Table~\ref{tab:sentdiv} shows the division of sentences for training and test.

% \begin{table}[t]
% \centering
% \begin{tabular}{l|l|c|c}
% & \multirow{2}{*}{\# Dialogues} & \multicolumn{2}{c}{\# Sentences} \\ \cline{3-4}
%  &  & \multicolumn{1}{l|}{with errors} & \multicolumn{1}{l}{without errors} \\ \hline
% Training Set & \multicolumn{1}{|c|}{19} & 4384 & 459 \\
% Test Set & \multicolumn{1}{|c|}{4} & 831 & 85 \\ \hline
% \end{tabular}
% \caption{Target domain training and test set for TBL}
% \label{tab:sentdiv}
% \end{table}


